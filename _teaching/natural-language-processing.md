---
title: "Natural Language Processing"
collection: teaching
type: "graduate course"
permalink: /teaching/natural-language-processing
venue: "National Kaohsiung University of Science and Technology"
date: 2021-09-01
location: "City, Country"
---


## Course Description
The course mainly introduces the theories and applications of Natural Language Processing (NLP). This course starts from basic concepts of NLP to the-state-of-the-art NLP algorithms such as transformer. Also, this course will introduce how to apply the concepts that learnt in this course to task-oriented dialog system.

## Lecture 1: Introduction to Natural Language Processing
* Lecture Slides: [PPT](../files/course_material/natural-language-processing/Lecture1.pptx)
* Reading Materials
  1. [Can We Automate Scientific Reviewing?](https://arxiv.org/abs/2102.00176)
  2. [The Design and Implementation of XiaoIce, an Empathetic Social Chatbot](https://dl.acm.org/doi/10.1162/coli_a_00368)
  3. [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637)
  4. [Semantically-Aligned Equation Generation for Solving and Reasoning MathWord Problems](https://www.aclweb.org/anthology/N19-1272/)
  5. [Stanza : A Python Natural Language Processing Toolkit for Many Human Languages](https://www.aclweb.org/anthology/2020.acl-demos.14/)
 
## Lecture 2: Distributed Representaion of Words
 * Lecture Slides: [PPT](../files/course_material/natural-language-processing/Lecture2.pptx)
 * Reading Materials
   1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
   2. [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)
   3. [Stanford University CS224N Lecture1 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

## Lecture 3: Advanced Word Vector and Introduction to Neural Network
* Lecture Slides: [PPT](../files/course_material/natural-language-processing/Lecture3.pptx)
* Reading Materials
  1. [GloVe: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162/)
  2. [Speech and Language Processing: Information Extraction](https://web.stanford.edu/~jurafsky/slp3/) (Chapter number may vary according to the edition)
  3. [Stanford University CS224N Lecture2 and Lecture3 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/) 

## Lecture 4: Backpropagation and Computation Graphs
* Lecture Slides: [PPT](../files/course_material/natural-language-processing/Lecture4.pptx)
* Reading Materials
  1. [Maxout Networks](https://arxiv.org/pdf/1302.4389.pdfMaxout%20Networks)
  2. [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
  3. [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)
  4. [Adam: A Method for Stochastic Optimization](https://arxiv.org/pdf/1412.6980.pdf)
  5. [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)
  6. [Stanford University CS224N Lecture4 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

## Lecture 5: Dependency Parsing
* Lecture Slides: [PPT](../files/course_material/natural-language-processing/Lecture5.pptx)
* Reading Materials
   1. [Incrementality in Deterministic Dependency Parsing](https://www.aclweb.org/anthology/W04-0308/)
   2. [Globally Normalized Transition-Based Neural Networks](https://arxiv.org/abs/1603.06042)
   3. [Fast and Robust Neural Network Joint Models for Statistical Machine Translation](https://www.aclweb.org/anthology/P14-1129/)
   4. [A Fast and Accurate Dependency Parser using Neural Networks](https://www.aclweb.org/anthology/D14-1082/)
   5. [Stanford University CS224N Lecture5 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)
* [Universal Dependency Parsing](https://universaldependencies.org/)

## Lecture 6: Recurrent Neural Networks and Language Models
* Lecture Slides: [PPT](../files/course_material/natural-language-processing/Lecture6.pptx)
 * Reading Materials
   1. [Speech and Language Processing: N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/) (Chapter number may vary according to the edition)
   2. [Deep Visual-Semantic Alignments for Generating Image Descriptions](https://arxiv.org/abs/1412.2306)
   3. [Speechless? Hereâ€™s how AI learns to finish your sentences](https://tech.fb.com/speechless-heres-how-ai-learns-to-finish-your-sentences/)
   4. [Stanford University CS224N Lecture6 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)
   5. [Deep Learning - Sequence Modeling: Recurrent and Recursive Nets](https://www.deeplearningbook.org/)

## Lecture 7: Vanishing Gradients and Fancy RNNs
* Lecture Slides: [PPT](../files/course_material/natural-language-processing/Lecture7.pptx)
* Reading Materials
  1. [On the difficulty of training Recurrent Neural Networks](https://arxiv.org/abs/1211.5063)
  2. [Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies](https://www.aclweb.org/anthology/Q16-1037/)
  3. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)
  4. [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
  5. [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)
  6. [Highway Networks](https://arxiv.org/abs/1505.00387)
  7. [Stanford University CS224N Lecture7 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

## Lecture 8: Machine Translation, Seq2Seq, Attention, and Transformer
* Lecture Slides: [PPT](../files/course_material/natural-language-processing/Lecture8.pptx)
* Reading Materials
    1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    2. [Attention Is All You Need](http://nlp.seas.harvard.edu/2018/04/03/attention.html) (Code Explanation)
    3. [Layer Normalization](https://arxiv.org/abs/1607.06450)
    4. [Universal Language Model Fine-tuning for Text Classification](https://aclanthology.org/P18-1031/)
    5. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423/)
    6. [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://www.aclweb.org/anthology/2020.acl-main.703/)
    7. [Stanford University CS224N Lecture8 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)
    8. [Stanford University CS224N Lecture13 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

## Lecture 9: Task Oriented Dialogue Systems & Multi-Modal Dialog System
* Lecture Slides: [PPT](../files/course_material/natural-language-processing/Lecture9.pptx)
* Reading Materials
  1.  [Speech and Language Processing: Chatbots and Dialogue Systems](https://web.stanford.edu/~jurafsky/slp3/) (Chapter number may vary according to the edition)
  2.  [Continuously Learning Neural Dialogue Management](https://arxiv.org/abs/1606.02689)
  3.  [Sample-efficient Actor-Critic Reinforcement Learning with Supervised Data for Dialogue Management](https://arxiv.org/abs/1707.00130)
  4.  [Augment Information with Multimodal Information](https://visualqa.org/workshop_2020.html)

## Textbooks
* Natural Language Processing
  * Jurafsky and Martin, [Speech and Language Processing (3rd ed.)](https://web.stanford.edu/~jurafsky/slp3/)
* Deep Learning
  * Goodfellow, Bengio, and Courville, [Deep Learning](https://www.deeplearningbook.org/)
  * Zhang et al., [Dive into Deep Learning](https://d2l.ai/)

## Online Courses
* Natural Language Processing and Deep Learning
  * [CS224n: Natural Language Processing with Deep Learning (2019 Edition)](http://web.stanford.edu/class/cs224n/)