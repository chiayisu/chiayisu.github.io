---
title: "Natural Language Processing Spring 2021"
collection: teaching
type: "Graduate course"
permalink: /teaching/2021-spring-teaching
venue: "National Kaohsiung University of Science and Technology"
date: 2021
location: "City, Country"
---



## Course Description
The course mainly introduces the theories and applications of Natural Language Processing (NLP). This course starts from basic concepts of NLP to the-state-of-the-art NLP algorithms such as transformer. Also, this course will introduce how to apply the concepts that learnt in this course to task-oriented dialog system and introduce some concepts of reinforcement learning if time-permitted.

## Lecture 1: Introduction to Natural Language Processing
* Lecture Slides: [PPT](https://drive.google.com/file/d/1e6f12ceaBz5YpIOa8dIBVPyO0fsJQ_O7/view?usp=sharing), [PDF](https://drive.google.com/file/d/18kzde9mnlyRezQ1Xz5wx_zGfB8PsTUze/view?usp=sharing)
* Reading Materials
  1. [Can We Automate Scientific Reviewing?](https://arxiv.org/abs/2102.00176)
  2. [The Design and Implementation of XiaoIce, an Empathetic Social Chatbot](https://dl.acm.org/doi/10.1162/coli_a_00368)
  3. [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637)
  4. [Semantically-Aligned Equation Generation for Solving and Reasoning MathWord Problems](https://www.aclweb.org/anthology/N19-1272/)
  5. [Stanza : A Python Natural Language Processing Toolkit for Many Human Languages](https://www.aclweb.org/anthology/2020.acl-demos.14/)

## Lecture 2: Introduction to Machine Learning and Text Classification
* Lecture Slides: [PPT](https://drive.google.com/file/d/1W3lZDmiOC1J4Al4zUf-dNbJOQvySzXXe/view?usp=sharing), [PDF](https://drive.google.com/file/d/1o1xTB8dEpYuFKnZHZr47wzw9-m1v4Dpv/view?usp=sharing)
* Reading Materials
  1. [A comparative study of decision tree ID3 and C4.5](https://www.researchgate.net/publication/265162251_A_comparative_study_of_decision_tree_ID3_and_C45)
  2. [Naive Bayes and Text Classification I Introduction and Theory](https://arxiv.org/pdf/1410.5329.pdf)
  3. [Clustering by Passing Message Between Data Points](https://www.icmla-conference.org/icmla07/FreyDueckScience07.pdf)
  4. [Affinity Propagation: A Clustering Algorithm for Computer-Assisted Business Simulations and Experiential Exercises](https://www.semanticscholar.org/paper/Affinity-Propagation%3A-A-Clustering-Algorithm-for-Thavikulwat/2c6487ca952e0297ba2b84dad6196a4d96b1d780)
  5. [Stanford University CS231N Lecture Notes on KNN](https://cs231n.github.io/classification/)
 ## Lecture 3: Distributed Representaion of Words
 * Lecture Slides: [PPT](https://drive.google.com/file/d/136bQeifOLz6j_Uxswy0e5p9bpOScPJJp/view?usp=sharing), [PDF](https://drive.google.com/file/d/1J8LAf5h1fy9YOP81zjFiJlfbHhy6PDtT/view?usp=sharing)
 * Reading Materials
   1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
   2. [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)
   3. [Stanford University CS224N Lecture1 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

## Lecture 4: Advanced Word Vector and Introduction to Neural Network
* Lecture Slides: [PPT](https://drive.google.com/file/d/1JIhWbyuF5e4zdtaDSlVkTBjPJC9MYuKl/view?usp=sharing), [PDF](https://drive.google.com/file/d/1BkFV_gcImeQGZAIt7tGhBQqyUJvy4X-K/view?usp=sharing)
* Reading Materials
  1. [GloVe: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162/)
  2. [Speech and Language Processing: Information Extraction](https://web.stanford.edu/~jurafsky/slp3/) (Chapter number may vary according to the edition)
  3. [Stanford University CS224N Lecture2 and Lecture3 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

## Lecture 5: Backpropagation and Computation Graphs
* Lecture Slides: [PPT](https://drive.google.com/file/d/1O9McrY5nlS-Q-PHusKuQc6iT6TprD8Ri/view?usp=sharing), [PDF](https://drive.google.com/file/d/1cCW6MJtjoQcgaUMdvok6ZbUx9mNZ-7K2/view?usp=sharing)
* Reading Materials
  1. [Maxout Networks](https://arxiv.org/pdf/1302.4389.pdfMaxout%20Networks)
  2. [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
  3. [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)
  4. [Adam: A Method for Stochastic Optimization](https://arxiv.org/pdf/1412.6980.pdf)
  5. [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)
  6. [Stanford University CS224N Lecture4 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

## Lecture 6: Dependency Parsing
* Lecture Slides: [PPT](https://drive.google.com/file/d/1COCLt4zUsMZKPz3V_TRIWmOjobHnj-er/view?usp=sharing), [PDF](https://drive.google.com/file/d/1H7SHzGuNDZ_FU5tljqUelK2Oh23SUGxh/view?usp=sharing)
* Reading Materials
   1. [Incrementality in Deterministic Dependency Parsing](https://www.aclweb.org/anthology/W04-0308/)
   2. [Globally Normalized Transition-Based Neural Networks](https://arxiv.org/abs/1603.06042)
   3. [Fast and Robust Neural Network Joint Models for Statistical Machine Translation](https://www.aclweb.org/anthology/P14-1129/)
   4. [A Fast and Accurate Dependency Parser using Neural Networks](https://www.aclweb.org/anthology/D14-1082/)
   5. [Stanford University CS224N Lecture5 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)
* [Universal Dependency Parsing](https://universaldependencies.org/)

## Lecture 7: Recurrent Neural Networks and Language Models
* Lecture Slides: [PPT](https://drive.google.com/file/d/1ygjjkDyO_iRQsKGUKGAQurx2frCFgjjR/view?usp=sharing), [PDF](https://drive.google.com/file/d/1vCHum6Kq_w6MGMSPaHDMkk_KMvlP9WMb/view?usp=sharing)
 * Reading Materials
   1. [Speech and Language Processing: N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/) (Chapter number may vary according to the edition)
   2. [Deep Visual-Semantic Alignments for Generating Image Descriptions](https://arxiv.org/abs/1412.2306)
   3. [Speechless? Hereâ€™s how AI learns to finish your sentences](https://tech.fb.com/speechless-heres-how-ai-learns-to-finish-your-sentences/)
   4. [Stanford University CS224N Lecture6 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)
   5. [Deep Learning - Sequence Modeling: Recurrent and Recursive Nets](https://www.deeplearningbook.org/)

## Lecture 8: Vanishing Gradients and Fancy RNNs
* Lecture Slides: [PPT](https://drive.google.com/file/d/1jFTtKhmpCwudrM9TbsB2MBnHdmVC6K17/view?usp=sharing), [PDF](https://drive.google.com/file/d/1hWh_lonH_7PR_DgnvwNQL-Iw6Q8ZHbSR/view?usp=sharing)
* Reading Materials
  1. [On the difficulty of training Recurrent Neural Networks](https://arxiv.org/abs/1211.5063)
  2. [Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies](https://www.aclweb.org/anthology/Q16-1037/)
  3. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)
  4. [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
  5. [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)
  6. [Highway Networks](https://arxiv.org/abs/1505.00387)
  7. [Stanford University CS224N Lecture7 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

## Lecture 9: Machine Translation, Seq2Seq, Attention, and Transformer
* Lecture Slides: [PPT](https://drive.google.com/file/d/1o93nLCGl9mubUGIPmLrCuyNuWJGUkGre/view?usp=sharing), [PDF](https://drive.google.com/file/d/1LDoS4D-Fbz2T0V3k1TxY16tQm3pexRTF/view?usp=sharing)
* Reading Materials
    1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    2. [Attention Is All You Need](http://nlp.seas.harvard.edu/2018/04/03/attention.html) (Code Explanation)
    3. [Layer Normalization](https://arxiv.org/abs/1607.06450)
    4. [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://www.aclweb.org/anthology/2020.acl-main.703/)
    5. [Stanford University CS224N Lecture8 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)
    6. [Stanford University CS224N Lecture13 (2019 Edition)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

## Lecture 10: Task Oriented Dialogue Systems & Multi-Modal Dialog System
* Lecture Slides: [PPT](https://drive.google.com/file/d/1Y0PBKbMGerHaoSN-s-8jAVaLCSt85WDr/view?usp=sharing), [PDF](https://drive.google.com/file/d/1Dxd5n44f4oiMQDC7EUdNmacmiNshaYRE/view?usp=sharing)
* Reading Materials
  1.  [Speech and Language Processing: Chatbots and Dialogue Systems](https://web.stanford.edu/~jurafsky/slp3/) (Chapter number may vary according to the edition)
  2.  [Continuously Learning Neural Dialogue Management](https://arxiv.org/abs/1606.02689)
  3.  [Augment Information with Multimodal Information](https://visualqa.org/workshop_2020.html)

## Machine Learning Algorithm Implementation
1. [Machine Learning Algorithm Implementation without Fancy Library](https://github.com/chiayisu/NLP_and_ML_Algorithm)

## Textbooks
* Dan Jurafsky and James H. Martin, [Speech and Language Processing (3rd ed.)](https://web.stanford.edu/~jurafsky/slp3/)
* Ian Goodfellow, Yoshua Bengio, Aaron Courville, [Deep Learning](https://www.deeplearningbook.org/)
* Zhang et al., [Dive into Deep Learning](https://d2l.ai/)
